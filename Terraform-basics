### steps to follow while writing code

00-providers.tf
(Terraform + provider requirements, AWS config)

01-backend.tf
*(S3/DynamoDB remote state setup - partial config)*

02-variables.tf
(Input variable declarations with validation)

03-locals.tf
(Computed values, naming conventions, tags)

04-Infrastructure Modules

05-outputs.tf

######################################################################
IAC - Practice of mng infra as code
why terra- iac   multicloud   hcl -simple
terra ur use = ec2 eks  s3 vpc 
terraform -v
Terraform v1.12.0                     =======latest
https://releases.hashicorp.com/terraform/

Provider: Plugin to interact with cloud or service APIs (AWS, Azure, etc.).
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

# AWS provider configuration
provider "aws" {
  region = "us-west-2"
}
# Helm provider configuration (for deploying to Kubernetes)
provider "helm" {
  kubernetes {
    host                   = aws_eks_cluster.example.endpoint
    cluster_ca_certificate = base64decode(aws_eks_cluster.example.certificate_authority.data)
    exec {
      api_version = "client.authentication.k8s.io/v1beta1"
      args        = ["eks", "get-token", "--cluster-name", aws_eks_cluster.example.name]
      command     = "aws"
    }
  }
}

provisioner = feature to execute command and scripts on resources already created
USEC- RUN SHELL       2 FILE COPY
LOCAL = COMMAND    = run commands or scripts on the machin = use- 1 run shell 2 ansible playbook run

REMOTE= INLINE["   "]  = to execute commands/Shell-scripts on a remote resource  
                                  run shell/install software nginx/apache2
resource "aws_instance" "web" {
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t2.micro"
  key_name      = "my-key"

  # local-exec: Runs on your local machine
  provisioner "local-exec" {
    command = "echo ${self.public_ip} > instance_ip.txt"
  }

  # remote-exec: Runs on the remote EC2 instance
  provisioner "remote-exec" {
    inline = [
      "sudo yum update -y",
      "sudo yum install -y nginx"
    ]

    connection {
      type        = "ssh"
      user        = "ec2-user"
      private_key = file("~/.ssh/id_rsa")
      host        = self.public_ip
    }
  }
}


 provisioner "file" {
    source      = "./myapp.conf"       # local file 
    destination = "/etc/myapp.conf"    # remote location
    connection { ssh 4 lines
}
}
Resource: Declares infrastructure component (VM, DB, etc.).

Module: Reusable CODE of Terraform configurations.
PUB - avail on registry
PVT - company module
ROOT - DIR terra confi directly applied
CHILD- that called from root
module "vpc_mod"{
source= "./path"
versions= "   "
}


State File= JSONFILE Stores current state of managed infrastructure.
TRACKES STATES
IDEMPOTENCY - AVOID REPEAT CREATION
CREATE& DELETE IN ORDER

- To restore your Terraform state from a backup file
mv terraform.tfstate.backup terraform.tfstate
terraform init
terraform plan
Rename backup to state file, then init and plan

conf file = .tf file
conf dir = dir with .tf files

T refresh = read current infra & update tfstate only  ( not conf file/ infra)

T import = bring manualy created existing infra state-file
        =Rebuild state when state file is lost or corrupted, allowing for recovery of existing resources without starting from scratch.
process - provider + conf file + init + import +plan to check must not add new ?
1——Write the Resource Block: 
resource "aws_instance" "example" {
}
2——--terraform import aws_instance.example i-1234567890abcdef0
3——Verify State: After importing, run terraform plan
Backend: Where Terraform stores state (local, S3, Azure Blob, etc.).

Plan: Preview of changes Terraform will make.

Apply: Executes changes to reach desired infrastructure state.

Destroy: Removes all managed infrastructure.

Variable: parameterize Input value for configuration (string, number, etc.).

Output: Display Value retrieved  from resource.

Workspace: feature allows work with multi env with same conf files
terraform apply --auto-approve -var "file=dev.tfvars"
USE - ENV COLLZB , PARALLEL WORK ENV
    - SEPARETE STATEFILE + TFVARS FILE

terra workspace  LIST 
                NEW PROD
                SELECT PROD
CANT DELETE W/S U CURRENTLY ON

Remote State: Storing state file in a shared, remote location.
cidr =Classless Inter-Domain Routing

terra apply -target='aws_instance.<resource_name>'



terraform {                             BKRDE   # cant use variable in provider block as it starts 1 st in code then vars
  backend "s3" {                          backend "local" {   path = "lacal dir path where store us tfstate" }
    bucket=
    key    =
    region  =
    dynamodb_table=
    encrypt  = true

aws_s3_bucket                                                                                   bucket
 bucket= 
acl=
versioning {
server_side_encryption_configuration {                 --- if above encrypt= true given then not need this arg here

aws_dynamodb_table                                                                              dynamodb_table

resource "random_id" "server" {                                                                 random_id.server.hex
  byte_length = 8
}

resource "aws_s3_object" "object" {                                                              object
  bucket = aws_s3_bucket.example.id
  key    = "ganja"
  source = "C:/Users/dhair/OneDrive/Desktop/TERRAFORM/S3/backend.tf"

LOCAL BACKEND =Custom path/workspace	Customization, workspace management

4—Environment Variables:
-export TF_VAR_filename="pet.txt"                                                               export TF_VAR_

unset aws_access_key_id    # removes it


calling variable value
 instance_type = var.instance_types[0]    OR
${var.username[0]}                 # change index number if list isthere


rsource "GitHub_repository" "myrepo"                                                           GitHub_repository

filename = "${path.module}/foo.bar"                                                             "${path.module}/foo.bar"

resource "tls_private_key" "rsa-4096-example" {                                                  tls_private_key




resource "local_file" "foo" {                                                                                             local_file

  content  =  tls_private_key.rsa-4096-example.private_key_pem####COPY NAME FROM ABOVE AND ADD private_key_pem
  filename = "pvt_key"              # filename = "${path.module}/foo.bar" —-this was line which i copy from terra
}


resource "aws_key_pair" "deployer" {                                                                                 aws_key_pair
public_key_openssh ----shevati lava                                                                                  public_key_openssh

terraform taint aws_instance.myinstance

user_data = base64encode(<<-EOF  
user_data = file("${path.module}/script.sh")                                                            file("${path.module}/script.sh")
check logs (e.g., /var/log/cloud-init.log) on the instance for troubleshooting.

${path.module}       ========== same folder

snap install kubectl --classic

provider "aws" {
  alias  = "us_west"
  region = "us-west-2"
}

resource "aws_instance" "east_instance" {                                                                             instance
  ami           = "ami-12345678"
  instance_type = "t2.micro"
  provider      = aws.us_east                      ###add provider = aws.ALIAS   # no alpviram         
}

terraform.lock.hcl         ## delete this file if want to update provider version

Create a New Profile:
aws configure --profile [profile_name]
List All Profiles:
cat ~/.aws/credentials
Use a Specific Profile:
aws s3 ls --profile [profile_name]
Set Default Profile Temporarily:
export AWS_PROFILE=[profile_name]
Check Current Profile:
echo $AWS_PROFILE

If you want to delete an instance without —target
-Remove from State: 
terraform state rm aws_instance.example["instance_key"]
Replace "instance_key" with the key of the instance you want to remove.
-Apply Changes:
terraform apply


.tf code test
terratest 
tflint

terraform module registry == public  registry where provider and community share modules

terra destroy --target=aws_instance.myec2

terra for infra
ansible for configuration and service

TERRA BLUE GREEN DEPLOYMENT===
-ADD 2 TG 
-1 FOR BLUE INFRA
-2 FOR GREEN INFRA SERVER
-3 SWITCH

multicloud==
need provider for each cloud
and creds

junior deletes statefiel
import every resource
create again terra aply
When using remote state, Terraform does not maintain a local state file by default. 
Instead, it stores the state information in a remote data store, which can be accessed by multiple users or systems. 
However, you can still pull the state locally if needed using the terraform state pull command, but this is typically for temporary access or manual modifications.


MY ERROR IN 
COUNT—- i use var.name to get number insead of giving direct value like 3 
FOR_EACH —- i use list(string) instead of set(string) / map(string)
- to pull its value i use var.name instead of each value


give name -1  name-2 name-3 must be with count angument---
name= "name-${count.index+1}  " 
 ####+1 add so number start from 1 not 0. and need ${}

count =4
filename= var.filename[count.index]            
or
count =length(var.filename)
filename= var.filename[count.index]

resource "null_resource" " {}
trigger {id= timestamp()}           # instance id
provisioner
-run 1 st only then run only if trigger value changes
- Null means resource does not exists on any cloud aws, azure gcp ....
- execute shell , ansible playbook , local m/c commands


TERRA IMPORT

SINGLE RESOUREC===
disaster recovery
1——-resource "aws_instance" "example" {}
2——--terraform import aws_instance.example i-1234567890abcdef0
3——Verify State: After importing, run terraform plan
-single resource import

MULTIPLE==
1—-import {
  to = "aws_instance.example_ec2"
  id = "i-0abcd1234efgh5678"  # Replace with your EC2 instance ID
}
2———-RESOURCE BLOCKS
resource "aws_instance" "example_ec2" {}
3———-terraform plan -generate-config-out=ImportResources.tf
4 Review the Generated Configuration: ImportResources.tf
5 copy code into main.tf from ImportResources.tf if u want
6 Deleting ImportResources.tf after verification and running terraform apply ensures that your infrastructure remains consistent without duplicating resource definitions
7 terraform apply -auto-approve

key        = "path/to/your/${terraform.workspace}/terraform.tfstate"

<<EOT                    CORRECT
<<---EOT                 NO NEED HYPHEN

KEY PAIR===
resource "aws_key_pair" "deployer" {
resource "tls_private_key" "rsa-4096-example" {
resource "local_file" "foo" {


vpc_security_group_ids:   launching instances in a non-default VPC.
terraform taint aws_instance.myinstance
it will mark this ec2 as tainted mean damaged and recreate it again when u teraform apply command agin 
use this in dev stage mostly
Undo: Use terraform untaint <resource_name>


user_data = file("${path.module}/script.sh")
user_data = file("script.sh")
ABOVE BOTH CODELINES SAME


resource "aws_instance" "web" {
  ami             = data.aws_ami.ubuntu.id
  instance_type   = "t2.micro"
  key_name        = aws_key_pair.deployer.key_name
  security_groups = [aws_security_group.allow_tls.id]
  tags = {
    Name = "jenkins"
  }
  provisioner "file" {
  source      = "C:/users/dhair/Downloads/ganja"
  destination = "/home/ubuntu/ganja"

  connection {
    type     = "ssh"
    user     = "ubuntu"
    # private_key_pem = "C/users/dhair/Downloads/terraform-key.pem"  
    private_key =  file("C:/users/dhair/Downloads/terraform-key.pem")  
    host     = self.public_ip
  }
}

provisioner "remote-exec" {
  inline = [
    "sudo apt-get update -y",
    "sudo apt-get install docker.io -y",
  ]
  connection {
    type        = "ssh"
    user        = "ubuntu" # Assuming a standard Ubuntu AMI
    private_key = "/c/Users/dhair/Downloads/terraform-key.pem" # Adjust path if needed
    host        = self.public_ip
  }
}
  subnet_id = aws_subnet.pub_sub_2.id
}


ERROR SOLUTION----
file provisioner made
1--  GIVE   C:/        everywhere to get file from pc root
2-- destination = "/home/ubuntu/ganja"  --local la file nav kahihi aso pan /home/ubuntu/ nantar je dilay tyach name chi file banate ec2  var
remote made
1--  "              "   ,              Every command inverted commamade + end la comma








### for copy local script , and run

resource "aws_instance" "web" {
  # ...

  provisioner "file" {
    source      = "script.sh"
    destination = "/tmp/script.sh"
  }

  provisioner "remote-exec" {
    inline = [
      "chmod +x /tmp/script.sh",
      "/tmp/script.sh args",
    ]
  }
}


#inline == to give command direct

#scripts== to give bash scripts start with #!/bin/bash






string{"ajit1, aj2, wina}  alphanumeric
number
bool
list——list(string) / list(number)
map——map(string)  / map(number)
set ==list but no duplicate value   —-set(string) / set(number)
objects 
tuple—-can use string, number, bool all datatype in combine with tuple


SUMMERY OF RUNTIME VARIABLE 
1) Command Line (-var, -var-file)=
terraform apply -var="instance_type=t2.micro" ####LASTLA  -var= "x=y"
2) *.auto.tfvars
3) terraform.tfvars
4) *.tfvar        (dev.tfvar)
5) Environment Variables (export TF_VAR_instance_type=t2.micro)####export la no symbol




state drifr and mv

resource in AWS changed. How can you find out who did this?--- cloudtrail

ou made a module. One Terraform code uses that module. But, now, you improved that module, but the "caller" code is not compatible with the new version of the module. How can you have both versions of the module in use?
----- You can have verions on your modules


why remote backend--
state lock 
security from hacker

how multiple resource u  make
before version .13 they use count
but now they use for each

Protect Sensitive Data: Mark variables (e.g., passwords, API tokens) as sensitive to prevent them from
 appearing in CLI output or logs, reducing the risk of accidental exposure
variable "db_password" {
  type        = string
  sensitive   = true
}

Why would you need "data" resources in Terraform?
Answer: To refer to resources that already exists in AWS. For example, list of AMIs in a region.

If you are asked to implement Terraform in a team or company where they have never used Terraform, what issues might you solve pre-emptively?
Set standars and best practices before you start coding. 
Also, you many want to import existing resources in Terraform before you start.

You are going to deploy similar resources in Development, Staging and Prod environments. 
no hard-coded variables and use .tfvars file per environment.

Can a module call another module?
Answer: Yes, it is not recommended

Which folder caches module codes locally?
Answer: .terraform


What negative impact does it have if you remove .terraform folder?
Answer: None. Terraform will simply download everything when you run terraform init command


Does Terraform have built-in functions?
Answer: Yes. Many. e.g. csvdecode.

How can you pass csv an argument to a module?
 Answer:  Use csvdecode function. That will read a csv file into a List of Maps. 
You can pass that directly to a module that accepts List of Maps (One input Variable)


Using Terraform , you have deployed 6 resources in AWS. However, a developer deleted on of them via AWS Console. Turns out that , that resource was not needed any way. How can you make your terraform code and terraform state sync up now?
Answer: 1. terraform state rm resource_name &
        2. remove the relevant part of the code that creates the deleted resource

How would switch between versions of Terraform?
Answer:
  mac brew install tfenv
  tfenv (switches between many versions super easily.)

  There are other ways , as well (e.g. using containers and aliases)

terraform fmt

Terraform init command do?
Answer: Pulls in any refernced modules

Answer: Yes. tflint (open source)

Answer: Yes. tfsec


How do you upgrade the version or providers and plugins and modules?
Answer: terraform init -upgrade

terraform apply. What will happen?
Answer: terraform will run terraform plan anyway before running terraform apply

Besides terraform.tfvars file, which other files does terraform load for variable values?
Answer: *.auto.tfvars


create ssh key using terraform:
 tls_private_key

. What is "Dynamic Block" in Terraform?
Answer: It is kind of like a for loop.


During terraform plan , a resource is successful (i.e. there are no issues with terraform plan),
but fails during provisioning (i.e. during "apply") , what happens to the resource in terraform state?
Answer: It is marked as "tainted"

user is using terraform version 0.11 or above?
Answer
terraform { required_version = "~> 0.11" }

Can you use filter inside a data block?
Answer: Yes.


Which will tell terraform to look for all module source lines and retrieves the module codes and report errors if can't find them ?
Answer: terraform get


If a null resource takes no action, what could possibly a use case for it?
null has trigger which made it  special
any changes to the contents of the Kubernetes config file or Kubernetes YAML
 will cause the command to rerun.


You want to run terraform plan. However, you want to point to a non-default state file. How?
Answer: -state=PATH

If you want 2 identical aws provider sections, then you have use :
 Answer:  alias keyword on the 2nd one.

Where is local copy of the modules saved?
Answer: .terraform folder


Is there way to automatically create a Readme.md file based on your terraform code?
Answer:  terraform-docs markdown . >> README.md
  terraform-docs is an open source tool.


Which workspace do you work in by default?
Answer: default
100. How do you know which workspaces you have in the first place?
Answer: terraform workspace list

# Terraform graph 
terraform graph -type=apply | dot -Tpng -o graph.png
terraform graph -type=apply | dot -Tpdf -o graph.pdf

#####################################################
link-    https://livingdevops.com/devops/20-scenario-based-terraform-questions-with-answers-for-devops-interviews/


corrupted state file?
# if s3 exists
-s3  = If using remote state storage like S3, you can restore from a previous version.

# If no s3 backup exists:
-terraform refresh  ==Run terraform refresh to update state with real infrastructure state
-terraform import ==Use terraform import to bring existing resources back under Terraform management



How do I ensure I don’t accidentally delete something in Terraform?
-prevent_destroy = true in lifecycle blocks to protect = critical resources
-terraform plan = before applying and carefully review the planned changes, especially deletions. 
-separate state files = For critical infrastructure,use separate state files & implement strict access controls by IAM roles
-Set up mandatory code reviews in your CI/CD pipeline for any infrastructure changes.
-##For our most critical infrastructure, we implement a “breakglass” procedure where emergency changes require approval from multiple team leads, and changes are tracked in a dedicated audit system.



weekly automated drift detection job that:

-terraform plan = Runs against all environments
-terraform import/refresh =When manual changes are made, use terraform import to bring resources under Terraform management, or terraform refresh to update state.
-documentation & report man changes to infra team via jira ==Sends reports of any drift to the infrastructure team
-Generates Jira tickets for reconciliation of any manual changes
-##Tracks drift metrics over time to identify problematic areas


What are the benefits of organizing a Terraform project using modules/workspaces?
module  ==code reuse

w/s== 
-code reuse and ensuring consistency.
-multiple env (dev, staging, prod) with same code while 
-keeping their states separate 

-Our module structure typically follows:
terraform/
├── modules/
│   ├── networking/
│   ├── database/
│   └── compute/
├── environments/
│   ├── dev/
│   ├── staging/
│   └── production/
└── global/
    ├── iam/
    └── dns/


 How do you manage secrets in Terraform?

-External secret storage:
 We use HashiCorp Vault or AWS Secrets Manager to store sensitive values, retrieving them at runtime using data sources
-Sensitive marking: We use the sensitive = true attribute for outputs containing sensitive data.
-CI/CD integration: Our pipeline securely injects secrets during deployment without persisting them.
-Encrypted state: We ensure remote state is encrypted at rest using server-side encryption in S3 


How do you implement multi-region, multi-account architecture in Terraform?

1)  multi-region==Provider aliases handle multiple regions:

provider "aws" {
  region = "us-east-1"
}
provider "aws" {
  alias  = "west"
  region = "us-west-2"
}

2) multiple accounts: Assume role functionality manages 
provider "aws" {
  region = "us-east-1"
  assume_role {
    role_arn = "arn:aws:iam::ACCOUNT_ID:role/OrganizationAccountAccessRole"
  }
}

#####################################################
### OLD NOTES POINTS FROM VPC 

- ${   } =  interpolation karay he 2 lagat only

SUBNET
-availability_zone       = data.aws_availability_zones.available_zones.names[0]       --U add this in subnet 
-map_public_ip_on_launch = true  / flase  --->Bydefault FALSE so as per public subnet true and pvt false  

RT
-pub rt with                 gateway_id = aws_internet_gateway.gw.id
-pvt rt with                 gateway_id = aws_nat_gateway.example.id
-cidr_block = "10.0.1.0/24"          #####  "0.0.0.0/0" kara for both nat amd igw 


# To ensure proper ordering, it is recommended to add an explicit dependency

VPC-- Add below 2 urself
-enable_dns_hostnames = true
-instance_tenancy     = "default" /"dedicated" / "hosts"  
"default"-----VPC share physical hardware with other AWS customers with isolation
"dedicated": all EC2 instances launched within this VPC will run on hardware dedicated solely to your AWS account.
"host": launch EC2 instances onto Dedicated Hosts, which are physical servers fully dedicated to your use.


-gateway_id = aws_internet_gateway.gw.id    ---whenever u use other resource name then DONT put it in "double inverted comma"
--------


terraform init -migrate-state
===-migrate-state flag tells Terraform to take the state data from where it was previously stored 
   -and move it to the location defined by your updated backend configuration.


- PACKER =  cli tool for build AWS AMI  for based on templates

DRIFT-
1. What is Terraform Drift?
occurs when the actual infra differs from state defined in Terraform configuration or the state file.

2. How do you detect drift in Terraform?
Run terraform plan             (compares desired state vs. actual state).
Use terraform refresh         to sync the state file with real-world resources.
Integrate drift detection in CI/CD pipelines.
Tools like Driftctl, Terraform Cloud, or AWS Config can automate detection.

3. What causes infrastructure drift?
Manual changes via cloud console/CLI.
External processes (auto-scaling, cloud provider updates).
State file corruption or misalignment.

4 How do you fix drift once detected?
terraform import                    to bring unmanaged resources under Terraform control.
terraform apply -refresh-only       to update state without changes.
Destroy and recreate resources if necessary.

5. Can you ignore drift for certain resources?
ASG- Set ignore_changes on desired_capacity if using dynamic scaling.
EC2- User Data/AMI updates not tracked in Terraform -Use ignore_changes for volatile attributes (e.g., ami, user_data).
SG - Use separate SGs for Terraform vs. AWS-managed rules.
AWS S3 Buckets
Manual bucket policy/ACL changes.
Versioning/encryption enabled outside Terraform.---Enable S3 bucket versioning IN terra

-resource "aws_instance" "example" {
  lifecycle {
    ignore_changes = [tags, ami]  # Prevents drift on these attributes
  }
}

How to Prevent AWS Drift?
✅ Enforce "No Manual Changes" policy (RBAC, AWS IAM boundaries).
✅ Use Remote State (S3 + DynamoDB locking) to avoid state corruption.
✅ Enable AWS Config to track configuration changes.
✅ Run terraform plan in CI/CD before deployments.
✅ Use ignore_changes for volatile AWS-managed attributes.


terraform show        ---> Displays the latest state stored in the default state file.

To list resources currently managed by Terraform, use:
terraform state list             like aws_instance.<resource-name>
terraform state show             Displays detailed information about a specific resource 
terraform state mv              dangerous dont use it unless very  much need 
Moves a resource from one address in the state file to another.
terraform state mv [options] <source> <destination>


we can give terraform.workspace like that if we want ---> "${terraform.workspace}"
LockId is must otherwise it will not allow state locking

if u dont give versions then auto version  will be applied , and version will be lock when we init , (~> 5.0) from 5.0.0 up to (but not including) 6.0.0 . aws current provider version 5.81
consistent and predictable behavior  by locking the versions of Terraform provider
When a provider version changes ---> terraform init -upgrade.
after terr init terraform.lock.hcl and .terraform file created

aliases --> interacting with multiple accounts, regions, or configurations within the same cloud provider.
null resource without triggers only once with triggers also u can give instance_id type or if u want to run always then give timestamp
if we want give different values to different environment like dev , qa etc then use it but it will change statefile
sensitive = true in outut hidden to prevent accidental exposure
how to use for loop in terraform
we use manually created own modules
terra plan also locks statefile , can disable this locking behavior with the -lock=false
T plan do not create new statefile
If versioning is enabled on your S3 bucket, each time you run terraform apply, a new version of the state file is created, 


Terraform vars (variables) are for external inputs (CLI,env-variables, .tfvars files),making configurations reusable across environments.
Locals (local values) are for internal computations within a module, reducing code duplication and improving readability.
In DevOps on AWS, vars allow for easy environment-specific changes (e.g., EC2 instance types per environment), 
while locals help maintain consistent naming conventions or complex calculations for AWS resources within a single module.
